# ğŸ“‹ TODO - Stock Details avec Vraies DonnÃ©es

Ce document liste **toutes les tÃ¢ches restantes** pour que la page Stock Details fonctionne avec de vraies donnÃ©es au lieu de donnÃ©es de test.

---

## âœ… DÃ©jÃ  ImplÃ©mentÃ© (Fonctionnel)

- âœ… ModÃ¨les de base de donnÃ©es (Prisma schema)
- âœ… Routes API backend complÃ¨tes
- âœ… Services backend fonctionnels
- âœ… Composants frontend React
- âœ… Hooks React Query avec cache
- âœ… Page StockDetailPageEnhanced intÃ©grÃ©e
- âœ… Documentation complÃ¨te
- âœ… Script de seeding pour donnÃ©es de test

---

## ğŸ”´ PRIORITÃ‰ CRITIQUE - Pour Production

### 1. ğŸ“Š Scraping des DonnÃ©es Historiques

**Objectif** : RÃ©cupÃ©rer l'historique de prix quotidien depuis la BRVM

**Fichier Ã  crÃ©er** : `backend/jobs/scrapeStockHistory.job.ts`

**TÃ¢ches** :
- [ ] Analyser la structure HTML de la page historique BRVM
- [ ] CrÃ©er un scraper pour extraire les donnÃ©es OHLCV (Open, High, Low, Close, Volume)
- [ ] ImplÃ©menter la logique de parsing et validation
- [ ] Sauvegarder dans la table `stock_history` via Prisma
- [ ] GÃ©rer les doublons (upsert sur `stock_ticker` + `date`)
- [ ] Planifier l'exÃ©cution quotidienne (ex: tous les jours Ã  18h30 aprÃ¨s clÃ´ture)

**Technologies** :
```typescript
import * as cheerio from 'cheerio';
import axios from 'axios';
import cron from 'node-cron';

// Exemple de structure
async function scrapeStockHistory(ticker: string, startDate: Date, endDate: Date) {
  const url = `https://www.brvm.org/...`; // URL Ã  dÃ©terminer
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  // Parser les donnÃ©es
  const historyData = [];
  $('table tr').each((i, row) => {
    // Extraire date, open, high, low, close, volume
  });

  // Sauvegarder dans DB
  for (const data of historyData) {
    await prisma.stockHistory.upsert({
      where: { stock_ticker_date: { stock_ticker: ticker, date: data.date } },
      update: { ...data },
      create: { ...data, stockId: stock.id }
    });
  }
}

// Planifier: tous les jours Ã  18:30
cron.schedule('30 18 * * *', async () => {
  // Scraper l'historique du jour pour toutes les actions
});
```

**Endpoints BRVM Ã  investiguer** :
- Page historique BRVM : https://www.brvm.org/fr/cours-actions
- API si disponible
- Alternative : Ã‰cofin, Bloomberg, Yahoo Finance

**Estimation** : 2-3 jours de dÃ©veloppement

---

### 2. ğŸ’° Scraping des DonnÃ©es Fondamentales

**Objectif** : RÃ©cupÃ©rer les ratios financiers et donnÃ©es fondamentales

**Fichier Ã  crÃ©er** : `backend/jobs/scrapeFundamentals.job.ts`

**TÃ¢ches** :
- [ ] Identifier les sources de donnÃ©es fondamentales :
  - [ ] Rapports annuels des entreprises (PDFs sur sites web)
  - [ ] BRVM (si disponible)
  - [ ] Agence Ã‰cofin
  - [ ] Sites web des entreprises cotÃ©es
- [ ] CrÃ©er un scraper pour chaque source
- [ ] Parser et normaliser les donnÃ©es (P/E, ROE, ROA, etc.)
- [ ] Valider et convertir les montants (millions, milliards)
- [ ] Sauvegarder dans `stock_fundamentals` et `company_info`
- [ ] Planifier l'exÃ©cution hebdomadaire (ex: chaque dimanche)

**DonnÃ©es Ã  collecter** :

**Pour StockFundamental** :
```typescript
{
  market_cap: number,        // Capitalisation boursiÃ¨re
  pe_ratio: number,          // Price to Earnings
  pb_ratio: number,          // Price to Book
  dividend_yield: number,    // Rendement dividende (%)
  roe: number,              // Return on Equity (%)
  roa: number,              // Return on Assets (%)
  profit_margin: number,     // Marge bÃ©nÃ©ficiaire (%)
  debt_to_equity: number,    // Dette / Capitaux propres
  revenue: number,           // Chiffre d'affaires
  net_income: number,        // BÃ©nÃ©fice net
  ebitda: number,           // EBITDA
  free_cash_flow: number,   // Free Cash Flow
  shares_outstanding: number, // Actions en circulation
  eps: number               // BÃ©nÃ©fice par action
}
```

**Pour CompanyInfo** :
```typescript
{
  description: string,       // Description de l'entreprise
  website: string,          // Site web officiel
  employees: number,        // Nombre d'employÃ©s
  founded_year: number,     // AnnÃ©e de crÃ©ation
  headquarters: string,     // SiÃ¨ge social
  ceo: string,             // PDG actuel
  industry: string         // Industrie/secteur
}
```

**Sources recommandÃ©es** :
1. **Sites web des entreprises** (Ã  scraper manuellement d'abord) :
   - SLBC : https://www.sicable-ci.com
   - SNTS : https://www.sonatel.sn
   - SGBC : https://www.societegenerale.ci
   - etc.

2. **Agence Ã‰cofin** (articles financiers) :
   - https://www.agenceecofin.com

3. **BRVM** (si donnÃ©es disponibles) :
   - https://www.brvm.org

**Planification** :
```typescript
// Tous les dimanches Ã  10h
cron.schedule('0 10 * * 0', async () => {
  await scrapeFundamentals();
});
```

**Estimation** : 3-5 jours (beaucoup de sources diffÃ©rentes)

---

### 3. ğŸ“° Scraping des ActualitÃ©s

**Objectif** : RÃ©cupÃ©rer automatiquement les actualitÃ©s liÃ©es aux actions

**Fichier Ã  crÃ©er** : `backend/jobs/scrapeStockNews.job.ts`

**TÃ¢ches** :
- [ ] Identifier les sources d'actualitÃ©s fiables :
  - [ ] Agence Ã‰cofin (principale source)
  - [ ] Jeune Afrique (section finance)
  - [ ] Le Soleil (SÃ©nÃ©gal)
  - [ ] FraternitÃ© Matin (CÃ´te d'Ivoire)
  - [ ] Sites des bourses (BRVM)
- [ ] CrÃ©er des scrapers pour chaque source
- [ ] ImplÃ©menter la dÃ©tection automatique du ticker dans l'article
- [ ] Extraire : titre, rÃ©sumÃ©, source, URL, date de publication
- [ ] Ã‰viter les doublons (vÃ©rifier URL ou hash du titre)
- [ ] Sauvegarder dans `stock_news`
- [ ] Planifier l'exÃ©cution toutes les 2 heures (actualitÃ©s frÃ©quentes)

**Exemple de scraper Ã‰cofin** :
```typescript
async function scrapeEcofinNews() {
  const url = 'https://www.agenceecofin.com/secteur-financier';
  const response = await axios.get(url);
  const $ = cheerio.load(response.data);

  const articles = [];
  $('.article-item').each((i, el) => {
    const title = $(el).find('h2').text().trim();
    const url = $(el).find('a').attr('href');
    const summary = $(el).find('.summary').text().trim();
    const date = $(el).find('.date').text().trim();

    // DÃ©tecter le ticker dans le titre
    const tickers = detectTickers(title + ' ' + summary);

    for (const ticker of tickers) {
      articles.push({
        stock_ticker: ticker,
        title,
        summary,
        source: 'Agence Ecofin',
        url,
        published_at: parseDate(date)
      });
    }
  });

  return articles;
}

function detectTickers(text: string): string[] {
  const knownTickers = ['SLBC', 'SNTS', 'SGBC', 'BOAM', ...];
  const found = [];

  for (const ticker of knownTickers) {
    if (text.includes(ticker) || text.includes(getCompanyName(ticker))) {
      found.push(ticker);
    }
  }

  return found;
}
```

**Planification** :
```typescript
// Toutes les 2 heures
cron.schedule('0 */2 * * *', async () => {
  await scrapeAllNews();
});
```

**Estimation** : 2-3 jours

---

### 4. ğŸ”§ Configuration des Jobs Cron

**Objectif** : Automatiser l'exÃ©cution des scrapers

**Fichier Ã  crÃ©er** : `backend/jobs/scheduler.ts`

**TÃ¢ches** :
- [ ] Installer `node-cron` : `npm install node-cron @types/node-cron`
- [ ] CrÃ©er un scheduler central
- [ ] Configurer les horaires optimaux :
  - Historique : aprÃ¨s clÃ´ture BRVM (18h30 GMT)
  - ActualitÃ©s : toutes les 2h
  - Fondamentaux : une fois par semaine (dimanche)
- [ ] Ajouter des logs pour monitoring
- [ ] GÃ©rer les erreurs et retry
- [ ] CrÃ©er un endpoint admin pour dÃ©clencher manuellement

```typescript
// backend/jobs/scheduler.ts
import cron from 'node-cron';
import { scrapeStockHistory } from './scrapeStockHistory.job';
import { scrapeFundamentals } from './scrapeFundamentals.job';
import { scrapeStockNews } from './scrapeStockNews.job';

export function startScheduler() {
  console.log('ğŸ“… DÃ©marrage du scheduler...');

  // Historique quotidien - 18h30 GMT (aprÃ¨s clÃ´ture BRVM)
  cron.schedule('30 18 * * 1-5', async () => {
    console.log('ğŸ“Š Scraping historique quotidien...');
    try {
      await scrapeStockHistory();
      console.log('âœ… Historique scraped');
    } catch (error) {
      console.error('âŒ Erreur scraping historique:', error);
    }
  });

  // ActualitÃ©s - toutes les 2 heures
  cron.schedule('0 */2 * * *', async () => {
    console.log('ğŸ“° Scraping actualitÃ©s...');
    try {
      await scrapeStockNews();
      console.log('âœ… ActualitÃ©s scraped');
    } catch (error) {
      console.error('âŒ Erreur scraping news:', error);
    }
  });

  // Fondamentaux - chaque dimanche Ã  10h
  cron.schedule('0 10 * * 0', async () => {
    console.log('ğŸ’° Scraping donnÃ©es fondamentales...');
    try {
      await scrapeFundamentals();
      console.log('âœ… Fondamentaux scraped');
    } catch (error) {
      console.error('âŒ Erreur scraping fondamentaux:', error);
    }
  });

  console.log('âœ… Scheduler dÃ©marrÃ© avec succÃ¨s');
}

// Dans server.ts
import { startScheduler } from './jobs/scheduler';

app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  startScheduler(); // DÃ©marrer les jobs
});
```

**Estimation** : 1 jour

---

## ğŸŸ¡ PRIORITÃ‰ MOYENNE - AmÃ©liorations

### 5. ğŸ“ˆ Indicateurs Techniques AvancÃ©s

**Objectif** : Ajouter RSI, MACD, Bandes de Bollinger au graphique

**Fichier Ã  crÃ©er** : `afribourse/src/utils/technicalIndicators.ts`

**TÃ¢ches** :
- [ ] ImplÃ©menter le calcul du RSI (Relative Strength Index)
- [ ] ImplÃ©menter le calcul du MACD (Moving Average Convergence Divergence)
- [ ] ImplÃ©menter les Bandes de Bollinger
- [ ] ImplÃ©menter les Moyennes Mobiles (20, 50, 200 jours)
- [ ] Ajouter un toggle dans StockChart pour afficher/masquer ces indicateurs
- [ ] CrÃ©er des lignes supplÃ©mentaires dans Recharts

**Exemple RSI** :
```typescript
export function calculateRSI(prices: number[], period: number = 14): number[] {
  const rsiValues: number[] = [];
  const gains: number[] = [];
  const losses: number[] = [];

  for (let i = 1; i < prices.length; i++) {
    const diff = prices[i] - prices[i - 1];
    gains.push(diff > 0 ? diff : 0);
    losses.push(diff < 0 ? Math.abs(diff) : 0);
  }

  for (let i = period - 1; i < gains.length; i++) {
    const avgGain = gains.slice(i - period + 1, i + 1).reduce((a, b) => a + b) / period;
    const avgLoss = losses.slice(i - period + 1, i + 1).reduce((a, b) => a + b) / period;

    const rs = avgGain / avgLoss;
    const rsi = 100 - (100 / (1 + rs));
    rsiValues.push(rsi);
  }

  return rsiValues;
}
```

**Estimation** : 2-3 jours

---

### 6. ğŸ“Š Comparaison avec l'indice BRVM

**Objectif** : Comparer la performance de l'action avec l'indice BRVM

**TÃ¢ches** :
- [ ] Scraper les valeurs quotidiennes de l'indice BRVM (dÃ©jÃ  modÃ¨le `MarketIndex`)
- [ ] CrÃ©er une route API `/api/market-index/brvm/history`
- [ ] Ajouter un toggle dans le graphique "Comparer avec BRVM"
- [ ] Afficher 2 lignes sur le mÃªme graphique (normalisÃ©es en %)
- [ ] Calculer la performance relative (alpha, beta)

**Estimation** : 2 jours

---

### 7. ğŸ¯ Support et RÃ©sistance Automatiques

**Objectif** : Calculer et afficher les niveaux de support et rÃ©sistance

**TÃ¢ches** :
- [ ] ImplÃ©menter l'algorithme de dÃ©tection des pivots
- [ ] Calculer les niveaux de support (plus bas locaux)
- [ ] Calculer les niveaux de rÃ©sistance (plus hauts locaux)
- [ ] Afficher des lignes horizontales sur le graphique
- [ ] Ajouter des labels avec les prix

**Estimation** : 2 jours

---

## ğŸŸ¢ PRIORITÃ‰ BASSE - Nice to Have

### 8. ğŸ“‹ Carnet d'Ordres (Order Book)

**Objectif** : Afficher les meilleurs ordres d'achat et de vente

**Conditions** : NÃ©cessite que la BRVM expose ces donnÃ©es

**TÃ¢ches** :
- [ ] VÃ©rifier si la BRVM fournit le carnet d'ordres
- [ ] CrÃ©er le modÃ¨le `OrderBook` dans Prisma
- [ ] Scraper ou rÃ©cupÃ©rer via API
- [ ] CrÃ©er le composant `OrderBookDisplay.tsx`
- [ ] Afficher bid/ask spread

**Estimation** : 3 jours (si donnÃ©es disponibles)

---

### 9. ğŸ“… Calendrier des Ã‰vÃ©nements

**Objectif** : Afficher les Ã©vÃ©nements importants (AG, rÃ©sultats trimestriels, dividendes)

**TÃ¢ches** :
- [ ] CrÃ©er le modÃ¨le `StockEvent` dans Prisma
- [ ] Scraper les calendriers des entreprises
- [ ] CrÃ©er le composant `EventCalendar.tsx`
- [ ] Afficher dans l'onglet "Vue d'ensemble"

**Estimation** : 2-3 jours

---

### 10. ğŸ’¹ Historique des Dividendes

**Objectif** : Afficher l'historique complet des dividendes versÃ©s

**TÃ¢ches** :
- [ ] CrÃ©er le modÃ¨le `DividendHistory` dans Prisma
- [ ] Scraper l'historique depuis les rapports annuels
- [ ] CrÃ©er le composant `DividendHistory.tsx`
- [ ] Afficher un graphique de l'Ã©volution des dividendes

**Estimation** : 2 jours

---

## ğŸ”§ Infrastructure & DevOps

### 11. ğŸ³ Dockerisation des Jobs

**Objectif** : Conteneuriser les jobs de scraping pour faciliter le dÃ©ploiement

**TÃ¢ches** :
- [ ] CrÃ©er un `Dockerfile.jobs`
- [ ] Configurer les variables d'environnement
- [ ] Tester en local
- [ ] DÃ©ployer sur serveur (ou service cloud)

**Estimation** : 1 jour

---

### 12. ğŸ“Š Monitoring & Alertes

**Objectif** : Surveiller les jobs et Ãªtre alertÃ© en cas d'Ã©chec

**TÃ¢ches** :
- [ ] Installer un systÃ¨me de monitoring (ex: PM2, Winston logs)
- [ ] Configurer des alertes email/Slack en cas d'erreur
- [ ] CrÃ©er un dashboard admin pour voir l'Ã©tat des jobs
- [ ] Logger les mÃ©triques : nombre d'actions scraped, erreurs, durÃ©e

**Estimation** : 2 jours

---

### 13. âš¡ Optimisation des Performances

**Objectif** : AmÃ©liorer les temps de rÃ©ponse et rÃ©duire la charge serveur

**TÃ¢ches** :
- [ ] Ajouter un cache Redis pour les donnÃ©es frÃ©quemment demandÃ©es
- [ ] Optimiser les requÃªtes Prisma (utiliser `select` au lieu de tout rÃ©cupÃ©rer)
- [ ] ImplÃ©menter la pagination pour les actualitÃ©s
- [ ] Lazy loading des onglets (charger seulement quand activÃ©)
- [ ] Prefetching intelligent des donnÃ©es

**Estimation** : 2-3 jours

---

## ğŸ“ Documentation & Tests

### 14. ğŸ§ª Tests AutomatisÃ©s

**Objectif** : Assurer la fiabilitÃ© du code

**TÃ¢ches** :
- [ ] Tests unitaires pour les services backend
- [ ] Tests d'intÃ©gration pour les routes API
- [ ] Tests des scrapers (avec mocks)
- [ ] Tests des composants React (React Testing Library)
- [ ] Tests E2E (Playwright ou Cypress)

**Estimation** : 5 jours

---

### 15. ğŸ“š Documentation Technique

**Objectif** : Documenter pour la maintenance future

**TÃ¢ches** :
- [ ] Documenter chaque scraper (sources, structure HTML)
- [ ] CrÃ©er un guide de contribution
- [ ] Documenter l'architecture complÃ¨te
- [ ] CrÃ©er un runbook pour le troubleshooting

**Estimation** : 2 jours

---

## ğŸ“Š RÃ©capitulatif des Estimations

| PrioritÃ© | TÃ¢che | Estimation | Statut |
|----------|-------|------------|--------|
| ğŸ”´ **CRITIQUE** | Scraping Historique | 2-3 jours | â³ Ã€ faire |
| ğŸ”´ **CRITIQUE** | Scraping Fondamentaux | 3-5 jours | â³ Ã€ faire |
| ğŸ”´ **CRITIQUE** | Scraping ActualitÃ©s | 2-3 jours | â³ Ã€ faire |
| ğŸ”´ **CRITIQUE** | Configuration Jobs Cron | 1 jour | â³ Ã€ faire |
| ğŸŸ¡ **MOYENNE** | Indicateurs Techniques | 2-3 jours | â³ Ã€ faire |
| ğŸŸ¡ **MOYENNE** | Comparaison BRVM | 2 jours | â³ Ã€ faire |
| ğŸŸ¡ **MOYENNE** | Support/RÃ©sistance | 2 jours | â³ Ã€ faire |
| ğŸŸ¢ **BASSE** | Carnet d'Ordres | 3 jours | â³ Ã€ faire |
| ğŸŸ¢ **BASSE** | Calendrier Ã‰vÃ©nements | 2-3 jours | â³ Ã€ faire |
| ğŸŸ¢ **BASSE** | Historique Dividendes | 2 jours | â³ Ã€ faire |
| ğŸ”§ **INFRA** | Dockerisation | 1 jour | â³ Ã€ faire |
| ğŸ”§ **INFRA** | Monitoring | 2 jours | â³ Ã€ faire |
| ğŸ”§ **INFRA** | Optimisations | 2-3 jours | â³ Ã€ faire |
| ğŸ“ **DOC** | Tests | 5 jours | â³ Ã€ faire |
| ğŸ“ **DOC** | Documentation | 2 jours | â³ Ã€ faire |

**Total PrioritÃ© CRITIQUE** : ~10-15 jours
**Total PrioritÃ© MOYENNE** : ~6-8 jours
**Total PrioritÃ© BASSE** : ~7-8 jours
**Total Infrastructure** : ~5-7 jours
**Total Documentation** : ~7 jours

**TOTAL GÃ‰NÃ‰RAL** : ~35-45 jours (7-9 semaines)

---

## ğŸ¯ Plan de Sprint RecommandÃ©

### Sprint 1 (2 semaines) - MVP Production
- âœ… Scraping Historique
- âœ… Scraping ActualitÃ©s
- âœ… Configuration Jobs Cron
- âœ… Tests de base
- **Objectif** : Page fonctionnelle avec donnÃ©es rÃ©elles

### Sprint 2 (2 semaines) - Fondamentaux & Optimisation
- âœ… Scraping Fondamentaux
- âœ… Optimisation des performances
- âœ… Monitoring de base
- **Objectif** : DonnÃ©es complÃ¨tes et systÃ¨me stable

### Sprint 3 (2 semaines) - Analyse AvancÃ©e
- âœ… Indicateurs techniques
- âœ… Comparaison BRVM
- âœ… Support/RÃ©sistance
- **Objectif** : Outils d'analyse pro

### Sprint 4+ (Au besoin) - Features AvancÃ©es
- Carnet d'ordres
- Calendrier Ã©vÃ©nements
- Historique dividendes
- Tests complets
- Documentation finale

---

## ğŸš€ DÃ©marrer ImmÃ©diatement

Pour commencer dÃ¨s maintenant avec les donnÃ©es de test :

```bash
# 1. GÃ©nÃ©rer le client Prisma
cd backend && npx prisma generate

# 2. Appliquer les changements DB
npx prisma db push

# 3. InsÃ©rer des donnÃ©es de test
npx ts-node scripts/seedStockDetails.ts

# 4. Lancer l'application
npm run dev
```

Puis dans le frontend :
```bash
cd afribourse
npm run dev
```

Naviguez vers une action (ex: SLBC) pour voir la page en action avec les donnÃ©es de test !

---

## ğŸ“ Support & Questions

Pour toute question sur l'implÃ©mentation :
1. Consultez `IMPLEMENTATION_STOCK_DETAILS.md` pour les dÃ©tails techniques
2. Consultez `DEPLOIEMENT_STOCK_DETAILS.md` pour le dÃ©ploiement
3. Consultez `PLAN_AMELIORATION_STOCK_DETAILS.md` pour le plan original

**Bon courage pour la suite ! ğŸš€**

---

**CrÃ©Ã© le** : 19 Novembre 2024
**DerniÃ¨re mise Ã  jour** : 19 Novembre 2024
